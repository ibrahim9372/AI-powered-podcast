{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d51892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain chromadb pandas\n",
    "# %pip install -qU langchain_community pypdf\n",
    "!pip install tf-keras\n",
    "!pip install groq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e734f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "base_path = r\"D:\\summer 2_nd\\Fake_yap\"\n",
    "file_name = \"history.pdf\"\n",
    "name = \"Fake_yap\"\n",
    "career = \"music history\"\n",
    "model_list= [\"compound-beta-mini\",\"llama-3.3-70b-versatile\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0681e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFLoader(f\"{base_path}\\{file_name}\")\n",
    "document = loader.load()\n",
    "docs= [pages for pages in  document if len(pages.page_content.strip()) != 1 or 0 ] #deleting any null page if any\n",
    "#-------------- creating chunks-------------------------------------\n",
    "splitters = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap =40,\n",
    "    separators=[\"\\n\\n\", \"\\n\", r\"(?<=[.!?])\\s\", \" \", \"\"]\n",
    ")\n",
    "chunks =[]\n",
    "for pages in docs:\n",
    "    chunks.append(pages.page_content)\n",
    "print(chunks[0])\n",
    "#------------------------Create embeddings--------------\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L12-v2\") \n",
    "embeddings = embedder.encode(chunks, show_progress_bar=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "client = chromadb.PersistentClient(path=r\"D:\\summer 2_nd\\chroma_embeddings\")\n",
    "\n",
    "collection = client.get_or_create_collection(name=name)\n",
    "\n",
    "collection.add(\n",
    "    documents=chunks,\n",
    "    embeddings= embeddings,\n",
    "    ids = [f\"page_{index}\" for index in range(len(chunks))]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff694b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the collection and embeddings from the chorma db \n",
    "client = chromadb.PersistentClient(path=r\"D:\\summer 2_nd\\chroma_embeddings\")\n",
    "collection = client.get_collection(name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b499fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some questions for retrieval purposes \n",
    "questions = [\n",
    "    # From the \"In The Beginning\" section\n",
    "    \"Why do you think early humans turned to music for rituals like weather prayers and hunting?\",\n",
    "    \"What role do early homemade instruments, such as drums made from animal skins, play in our understanding of communal music making?\",\n",
    "\n",
    "    # From the \"Middle Ages\" section\n",
    "    \"How did naming musical notes and creating a writing system by Pope Gregory I around 600 A.D. transform the reach and power of music?\",\n",
    "    \"What impact did Gregorian chant—a simple melody sung in unison—have on spiritual and communal experience?\",\n",
    "    \"What drove the evolution from plainchant to elaborate masses involving choirs and orchestras?\",\n",
    "\n",
    "    # From the \"Renaissance\" section\n",
    "    \"How did the shift from monophonic chant to polyphonic madrigal reflect broader cultural and artistic developments?\",\n",
    "    \"In what ways did religious patronage—like that of Palestrina by the Pope—influence Renaissance musical creativity?\",\n",
    "    \"How did the madrigal lay the groundwork for the emergence of opera during the Renaissance?\",\n",
    "    \"What innovations did Monteverdi bring by combining memorable melodies, stage storytelling, and instrumental accompaniment?\",\n",
    "    \"Why do you think opera became the dominant form of musical entertainment in the late Renaissance?\",\n",
    "\n",
    "    # From the \"Baroque Era\" section\n",
    "    \"Baroque music is known for its ornamentation and expressive melodies—what do you think this says about the era’s artistic values?\",\n",
    "    \"Vivaldi’s 'Four Seasons' paints musical pictures of nature—how do you think music can convey feelings or scenes without any words?\",\n",
    "    \"Handel transitioned from opera to oratorio—how did using biblical texts and orchestras help connect with larger audiences?\",\n",
    "    \"Why do you think Handel’s 'Messiah' remains one of the most enduring oratorios in music history?\",\n",
    "    \"Bach was known for improvising on the organ and composing complex fugues—how does his approach show the balance between structure and creativity?\",\n",
    "    \"How might growing up with 20 children copying music in the Bach household have influenced his productivity and legacy?\",\n",
    "    \"What makes a fugue—with multiple melodies layered at different times—so uniquely challenging and impressive to perform?\",\n",
    "\n",
    "    # From the \"Classical Era\" section\n",
    "    \"Why do you think the Classical Era moved toward simpler, more structured music compared to the complex Baroque style?\",\n",
    "    \"Joseph Haydn is known as the father of the symphony—how did his innovations shape the future of orchestral music?\",\n",
    "    \"What role did Vienna and Austria play in shaping Classical music through composers like Haydn, Mozart, and Beethoven?\",\n",
    "    \"How did the values of the Classical era—clarity, balance, and form—reflect broader Enlightenment thinking?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "683bfacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Why do you think early humans turned to music for rituals like weather prayers and hunting?', 'What role do early homemade instruments, such as drums made from animal skins, play in our understanding of communal music making?', 'How did naming musical notes and creating a writing system by Pope Gregory I around 600 A.D. transform the reach and power of music?']\n"
     ]
    }
   ],
   "source": [
    "some_query = questions[:3]\n",
    "print(some_query)\n",
    "results = collection.query(\n",
    "    query_texts= some_query, \n",
    "    n_results=2,\n",
    "    include=[\"documents\", \"embeddings\", \"distances\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f74f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f46f5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "career = \"music history\"\n",
    "guest_name =\"wobily wiggly\"\n",
    "\n",
    "persona_prompt_podcaster= \"you are a famous and friendly youtube podcaster who is asking questions from your guest whos name is {guest_name}   and dont give visual cues and just ask 1 question a time \"\n",
    "persona_prompt_guest = f\"you are a friendly {career} professional answering questions from a famous podcaster, having a friendly podcast convesation  and dont give visual cues\"\n",
    "welcome_prompt = f\"Welcome the guess to your podcast who is a {career} professional.\"\n",
    "thanks_prompt = f\"Thanks the podcsater for having you on the show\"\n",
    "bye_prompt_podcaster = f\"Say thankyou and good bye to the {career} professional who came on your show\"\n",
    "bye_prompt_guest = f\"Say thank you and good bye to the podcaster who had you on the show\"\n",
    "def ask_q(question):\n",
    "    question_asking_baseline = f\"\"\"ask the question from the guest and repeat it inyour words\n",
    "    example : Let me ask you what is the meaning of RAG?\n",
    "    example2:  so can you tell me about dinosours\n",
    "\n",
    "    your question is : {questions}\n",
    "    \"\"\"\n",
    "    return question_asking_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eef27dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "groq_api = os.getenv(\"groq_api\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "01bb6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "client = Groq(api_key=groq_api)\n",
    "history = []  # Properly formatted history as a list of message dicts\n",
    "\n",
    "def podcaster_speaks(history, prompt):\n",
    "    global persona_prompt_podcaster\n",
    "\n",
    "    # Wrap the new user message\n",
    "    messages = [{\"role\": \"system\", \"content\": persona_prompt_podcaster}] + history + [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    podcaster = client.chat.completions.create(\n",
    "        model=model_list[0],\n",
    "        messages=messages,\n",
    "        temperature=0.5,\n",
    "        max_completion_tokens=300,\n",
    "        top_p=1,\n",
    "    )\n",
    "\n",
    "    reply = podcaster.choices[0].message\n",
    "    history.append({\"role\": reply.role, \"content\": reply.content})\n",
    "    return reply.content\n",
    "\n",
    "def podcaster_questions(history, prompt):\n",
    "    global persona_prompt_podcaster\n",
    "    question = ask_q(prompt)\n",
    "    messages = [{\"role\": \"system\", \"content\": persona_prompt_podcaster}] + history + [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    podcaster = client.chat.completions.create(\n",
    "        model=model_list[0],\n",
    "        messages=messages,\n",
    "        temperature=0.5,\n",
    "        max_completion_tokens=300,\n",
    "        top_p=1,\n",
    "    )\n",
    "\n",
    "    reply = podcaster.choices[0].message\n",
    "    history.append({\"role\": reply.role, \"content\": reply.content})\n",
    "    return reply.content\n",
    "\n",
    "def guest_speak(history, prompt):\n",
    "    global persona_prompt_guest\n",
    "\n",
    "    # Step 1: Get the most relevant chunk from ChromaDB\n",
    "    results = collection.query(\n",
    "        query_texts=[prompt],\n",
    "        n_results=1,\n",
    "        include=[\"documents\"]\n",
    "    )\n",
    "\n",
    "    context = results['documents'][0][0] if results['documents'][0] else \"No relevant document found.\"\n",
    "\n",
    "    # Step 2: Set system prompt to instruct model to use only the context\n",
    "    strict_system_prompt = f\"\"\"{persona_prompt_guest}\n",
    "\n",
    "Only answer using the following document. Do not make up or infer any information that is not explicitly stated but you can change the wording and make it look like you are saying it \n",
    "---\n",
    "{context}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "    # Step 3: Combine with chat history\n",
    "    messages = [{\"role\": \"system\", \"content\": strict_system_prompt}] + history + [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Step 4: Call the model\n",
    "    guest = client.chat.completions.create(\n",
    "        model=model_list[0],\n",
    "        messages=messages,\n",
    "        temperature=0.0,  # Reduce creativity for stricter grounding\n",
    "        max_completion_tokens=300,\n",
    "        top_p=1\n",
    "    )\n",
    "\n",
    "    # Step 5: Update history and return reply\n",
    "    reply = guest.choices[0].message\n",
    "    history.append({\"role\": reply.role, \"content\": reply.content})\n",
    "    return reply.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "074c899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Podcaster Turn -----------\n",
      "\n",
      "What's up everyone, welcome back to our channel, today I'm super excited to have with me, Dr. Rachel Kim, a renowned music history professional, Rachel thanks for being on the show! \n",
      "\n",
      "Can you tell us, what sparked your interest in music history, and how did you become an expert in this field?\n",
      "\n",
      "----------- Guest Turn -----------\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k0exqfqpf3nsha8v8p5n45jc` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99811, Requested 1050. Please try again in 12m23.366s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(p)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----------- Guest Turn -----------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m q \u001b[38;5;241m=\u001b[39m guest_speak(history, thanks_prompt)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(q)\n\u001b[0;32m     16\u001b[0m welcome \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[58], line 72\u001b[0m, in \u001b[0;36mguest_speak\u001b[1;34m(history, prompt)\u001b[0m\n\u001b[0;32m     67\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: strict_system_prompt}] \u001b[38;5;241m+\u001b[39m history \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m     68\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[0;32m     69\u001b[0m ]\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Step 4: Call the model\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m guest \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     73\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_list[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     74\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m     75\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,  \u001b[38;5;66;03m# Reduce creativity for stricter grounding\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     max_completion_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[0;32m     77\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     78\u001b[0m )\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Step 5: Update history and return reply\u001b[39;00m\n\u001b[0;32m     81\u001b[0m reply \u001b[38;5;241m=\u001b[39m guest\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:368\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    370\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    371\u001b[0m             {\n\u001b[0;32m    372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    373\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    374\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: exclude_domains,\n\u001b[0;32m    375\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_domains,\n\u001b[0;32m    379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    386\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    387\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[0;32m    389\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    390\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m: search_settings,\n\u001b[0;32m    391\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    392\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    393\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    394\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    395\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    396\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    397\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    398\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    399\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    400\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    401\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    402\u001b[0m             },\n\u001b[0;32m    403\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    404\u001b[0m         ),\n\u001b[0;32m    405\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    406\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    407\u001b[0m         ),\n\u001b[0;32m    408\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    409\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    410\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    411\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1232\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1220\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1228\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1229\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1230\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1231\u001b[0m     )\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1031\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1033\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k0exqfqpf3nsha8v8p5n45jc` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99811, Requested 1050. Please try again in 12m23.366s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "welcome = False\n",
    "questions_count = 0\n",
    "question_index = 0\n",
    "all_questions = questions[:4]  # your question list\n",
    "\n",
    "while True:\n",
    "    if not welcome:\n",
    "        print(\"----------- Podcaster Turn -----------\\n\")\n",
    "        p = podcaster_speaks(history, welcome_prompt)\n",
    "        print(p)\n",
    "\n",
    "        print(\"\\n----------- Guest Turn -----------\\n\")\n",
    "        q = guest_speak(history, thanks_prompt)\n",
    "        print(q)\n",
    "\n",
    "        welcome = True\n",
    "\n",
    "    elif questions_count < len(all_questions):\n",
    "        print(\"\\n----------- Podcaster Turn -----------\\n\")\n",
    "        p = podcaster_questions(history, all_questions[question_index])\n",
    "        print(p)\n",
    "\n",
    "        print(\"\\n----------- Guest Turn -----------\\n\")\n",
    "        q = guest_speak(history, all_questions[question_index])\n",
    "        print(q)\n",
    "\n",
    "        question_index += 1\n",
    "        questions_count += 1\n",
    "\n",
    "    else:\n",
    "        print(\"\\n----------- Podcaster Turn -----------\\n\")\n",
    "        p = podcaster_speaks(history, bye_prompt_podcaster)\n",
    "        print(p)\n",
    "\n",
    "        print(\"\\n----------- Guest Turn -----------\\n\")\n",
    "        q = guest_speak(history, bye_prompt_guest)\n",
    "        print(q)\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
